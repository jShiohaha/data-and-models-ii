\usepackage[obeyspaces]{url}

---
title: "Problem Set 5"
author: "Jacob Miller, Jacob Shiohira, Reid Gahan"
date: "4/14/2018"
output: pdf_document
---

```{r echo = FALSE, messages=FALSE, warning=FALSE, include=FALSE}
# NOTE: You must first install the following packages for your machine's R installation before this RMD file will compile
library(mice)
library(lattice)
library(dplyr)
library(corrplot)
library(pscl)
library(Metrics)
library(caret)
library(ROCR)
library(sigmoid)

workingDirectory = "/Users/jacobshiohira/Documents/ACADEMICS/JUNYA/SPRING/DATA-MODELS-II/PROBLEM-SETS/PROBLEM-SET-5"

setwd(workingDirectory)
# Include Keck Lib Utility Functions
source("KeckLib.R")
ECHO = TRUE
EVAL = TRUE
OUT = "750px"
SMALL = "350px"
MEDIUM = "550px"
LARGE = "750px"
date()
```

\subsection*{Problem 1}
```{r echo = TRUE}
# - Read in the Framinham data set
df <- read.csv("Data/framingham.csv", stringsAsFactors = FALSE)

# - How many observations? How many features (explanatory variables)?
str(df)
```

The output above tells us there are a total of $4,240$ observations and $16$ features in the dataset. The total number of observations in this case includes rows with possible NAN values.

\newpage

```{r echo = TRUE}
summary(df)

# - Explore the data (plots, tables, statistics) including feature types.
M <- cor(mtcars)
corrplot(M, method = "circle")
# TODO: Uncomment at the end... it just takes a while to run.
# pairs(df)

df_as_numeric <- sapply(df, as.numeric)
hist(df_as_numeric, freq=FALSE, xlab="Dataframe values", main="Distribution of Dataset Values", col="lightgreen")

# Count the total number of rows with at least one NAN value
df_nonnan <- na.omit(df)
num_null_rows <- nrow(df) - nrow(df_nonnan)
```


```{r echo = FALSE}
psi("Total number of rows with at least one NAN value: ", num_null_rows)
```


```{r echo = TRUE}
# - Summarize any NA's by feature
na_count <-sapply(df, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
```

\noindent
Below is a table outlining the number of NAN Values per feature. Note that the sum of NAN values across features is greater than the total number of rows with at least one NAN value because a single row could have multiple NAN values.

```{r echo = FALSE}
na_count
```

\subsection*{Problem 2}
```{r echo = TRUE}
n <- nrow(df)

# Documentation: https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice
# Using mice: http://web.maths.unsw.edu.au/~dwarton/missingDataLab.html, search for "pmm" method
# NOTE: changing the maxit param will affect the time it takes for mice() to run
imputed_df <- complete(mice(df, m=5, maxit=50, meth="pmm", printFlag=FALSE, seed=101))
summary(imputed_df)
```

```{r echo = TRUE}
# -- Uniformly then Normally scaling the dataset. Reference the histogram plots to verify the best type of scaling, if even necessary.
UnifDF <- ScaleUnif(imputed_df)
NormDF <- data.frame(apply(imputed_df, MARGIN = 2, FUN = function(X) (X - min(X))/diff(range(X))))
```

```{r echo = TRUE}
# -- Shuffle the dataset then split data with 80% in the training set and 20% in the test set.
set.seed(1)
shuffled_df <- NormDF[sample(n),]
trainset <- shuffled_df[1:round(0.8 * n), ]
testset <- shuffled_df[(round(0.8 * n) + 1):n, ]

# Baseline model - predict the mean of the training data with the TenYearCHD binary response variable
trainset_mean <- mean(trainset$TenYearCHD)

# Evaluate RMSE and MAE on the testing data
RMSE_baseline <- sqrt(mean((trainset_mean-testset$TenYearCHD)^2))
psp("Baseline RMSE: ", RMSE_baseline, 2)

MAE_baseline <- mean(abs(trainset_mean-testset$TenYearCHD))
psp("Baseline Mean Absolute Error: ", MAE_baseline, 2)

# Apply the "activation function" to the mean of the training set to use in all of the statistics
trainset_mean <- round(trainset_mean)

# -- What is the postive predictive rate (precision) of your baseline prediction ?
TenYearCHDAsTable <- table(testset$TenYearCHD)

truepositive <- TenYearCHDAsTable[names(TenYearCHDAsTable)==trainset_mean]
falsepositive <- TenYearCHDAsTable[names(TenYearCHDAsTable)!=trainset_mean]

psp("Postive Predictive Rate (precision): ", (truepositive / (truepositive + falsepositive)), 2)

baseline_accuracy <- Metrics::accuracy(testset$TenYearCHD, trainset_mean)
psp("Baseline Accuracy: ", baseline_accuracy, 2)
```

\noindent
\textbf{Question}: Which error type seems worse and thus should be considered along with accuracy in your predictions below? Why? \\

\noindent
The worse type of error here is not predicting cancer when there is actually cancer. It is possible that a model could have very high accuracy but just guess all $0$'s, as demonstrated by our baseline model. However, the importance in an application like this is the ability to precisely predict positive cases. Thus, the precision value, or Postive Predictive Rate, should also cbe considered alongside accuracy in the predictions below. 

\subsection*{Problem 3}
\noindent
A note on the output from \path{anova(...)}. The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time.

```{r echo = TRUE}
# Logistic Regression: http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html

# - Create a 'NULL' logistic regression model with glm() using only the intercept feature. Use glm's binomial family with the logit link.
modelNull <- glm(TenYearCHD ~ 1, family=binomial(link='logit'), data=trainset)
summary(modelNull)

modelNullAIC <- AIC(modelNull)
psi("Null Model AIC: ", modelNullAIC)

modelNullLogLike <- logLik(modelNull)
psi("Null Model Log likelihood: ", modelNullLogLike)

modelNullDeviance <- modelNull$deviance
psi("Null Model Deviance: ", modelNullDeviance)

# - Create a 'FULL' logistic regression model with glm() using all of the features including the intercept
modelFull <- glm(TenYearCHD ~., family=binomial(link='logit'), data=trainset)
summary(modelFull)

# Calculates and outputs logLik(model), model$deviance, and AIC(model) for full model
modelFullAIC <- AIC(modelFull)
psi("Full Model AIC: ", modelFullAIC)

modelFullLogLike <- logLik(modelFull)
psi("Full Model Log likelihood: ", modelFullLogLike)

modelFullDeviance <- modelFull$deviance
psi("Full Model Deviance: ", modelFullDeviance)

# - Compare the models and comment
# TODO...
```

\subsection*{Problem 4}
```{r echo = TRUE}
# - Create the lowest AIC ('best') model using backward elimination of parameters.
modelBest <- step(modelFull, direction = c("backward"), trace = 0)

# - Print the summary(), the log likelihood, the deviance, and the AIC for this model and compare with the previous two models.
summary(modelBest)

modelBestLogLike <- logLik(modelBest)
psi("Full Model Log likelihood: ", modelBestLogLike)

modelBestAIC <- AIC(modelBest)
psi("Full Model AIC: ", modelBestAIC)

# - Print these 'best' parameters (coefficients) along with their 95% confidence intervals in a single table
cbind(coef(modelBest), suppressMessages(confint(modelBest)))
```

\subsection*{Problem 5}
```{r echo = TRUE}
activationFunction <- function(x)(ifelse(x >= 0.5, 1, 0))

# - Using the 'best' model, “predict” the 10YrCHD on the training set.
trainingSetPredictions <- activationFunction(predict(modelBest, newdata=trainset, type="response"))
trainingSetPredictions <- sigmoid::sigmoid(predict(modelBest, newdata=trainset), method = "logistic", inverse = FALSE, SoftMax = FALSE)

# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates. Displayed using output function from KeckLib.R
t = table(actual = trainset$TenYearCHD, predict = trainingSetPredictions > 0.5)
acc(t, FALSE)
```

```{r echo = TRUE}
# - Compare to the baseline model from Question 2
# TODO...

# - Using the 'best' model, predict the 10YrCHD on the test set
# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates
testSetPredictions <- sigmoid::sigmoid(predict(modelBest, newdata=testset, type="response"), method = "logistic", inverse = FALSE, SoftMax = FALSE)

# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates. Displayed using output function from KeckLib.R
t = table(actual = testset$TenYearCHD, predict = testSetPredictions > 0.5)
acc(t, FALSE)
```

\subsection*{Problem 6}
```{r echo = TRUE}
# Now use ROC curves to reconsider a threshold (cutoff) of .5 using the ROCR (or other) package
# -- Create the prediction object for the training set
rocrPred <- ROCR::prediction(data.frame(trainingSetPredictions)[,1], trainset$TenYearCHD)

# - Plot the accuracy v. the threshold (cutoff)
perf <- ROCR::performance(rocrPred, measure = "acc") 
plot(perf, colorize=F, colorize.palette=rev(rainbow(256)))
grid()

# - Plot the true positive rate v. the false positive rate and label the threshold values
tprFprPerf <- ROCR::performance(rocrPred, measure = "tpr", x.measure = "fpr") 
plot(tprFprPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()

# - Plot sensitivity v. specificity and label the threshold values
sensVsSpecPerf <- ROCR::performance(rocrPred, measure = "sens", x.measure = "spec") 
plot(sensVsSpecPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()

# - Plot precision v. recall and label the threshold values
precVsRecallPerf <- ROCR::performance(rocrPred, measure = "prec", x.measure = "rec") 
plot(precVsRecallPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()

# - Comment on your findings for each plot
# TODO...

# - TODO: Compute AUC for the three models (null, full, and best) and comment

```

\subsection*{Problem 7}
```{r echo = TRUE}
# Using gradient descent, solve for the parameters from the 'best backward eliminaton' data set and verify relative to the glm() parameters
hypothesisVector = function(X, theta) {
  # sigmoid of the dot product between X and theta
  return ( as.vector( 1 / (1+exp(-(X %*% theta))) ))
}

Jcost = function(y, h) {
  epsilon = 1.e-10
  n = length(y)

  # Take the error when label=0
  ifelse(h < epsilon, epsilon, h)
  # Take the error when label=1
  ifelse(h > 1-epsilon, 1-epsilon, h)

  return ( (sum(y*log(h)) + sum((1-y)*log(1-h))) / n )
}

# TODO: does not converge correctly
gradientDescent <- function(X, y){
  X = as.matrix(X)
  y = as.vector(y)
  
  theta = runif(ncol(X), -1, 1)
  alpha = 0.5
  maxIterations = 10000
  n = length(y)
  epsilon = 1.e-10
  
  for ( i in 1:maxIterations){
      hypothesis = hypothesisVector(X, theta)
      # psi("cols: ", length(hypothesis))

      cost = Jcost(y, hypothesis)
      loss = hypothesis - y
      
      # avg gradient per example
      gradient = (t(X) %*% as.matrix(loss)) / n

      # update
      theta = theta - alpha * gradient

      if (i %% 100 == 0) {
        sse = sum(loss^2)

        psi("Iteration: ", i)
        psi("SSE: ", sse)
        # print(cost)
        # print(theta)
      }

      if(sum(loss)^2 < epsilon){
        break
      }
  }
  
  psi("Total number of iterations: ", i)
  return (theta)
}

# - Create the X and y test and training sets
trainY <- data.frame(trainset$TenYearCHD)
trainX <- trainset[,c("male", "age", "cigsPerDay", "prevalentStroke", "prevalentHyp", "sysBP", "glucose")]

testY <- testset$TenYearCHD
testX <- testset[,c("male", "age", "cigsPerDay", "prevalentStroke", "prevalentHyp", "sysBP", "glucose")]

# - Compute the parameters (coefficients)
thetas <- gradientDescent(trainX, trainY)

# - Compare to glm()'s coefficients in a single table
cbind(thetas, modelBest$coefficients)
```

\subsection*{Problem 8}
```{r echo = TRUE}
# TODO: finish these and make sure they match output from glm()

# -- Compute the following from the gradient descent code in the previous problem
# - standard errors
# - Z-statistics
# - P values in a single labeled table along with the coefficients
# - The standard errors are the diagonals of C =(X^T*V*X)^-1
# - and v_jj = p_j * (1-p_j) and v_ij = 0 for i /= j, 1 <= i,j <= p
# - p are the theta_hat probabilities before thresholding but after the sigmoid transformation
# - Compute and verify the log likelihood, deviance, and AIC with those from glm()

calculate_ssr <- function(actual, predictions){
  # returns value residual sum of squared errors
  ssr = sum((actual - predictions)^2)
  return (ssr)
}

calculate_sse <- function(actual, predictions){
    # returns value sum of squared errors
    ybar = mean(actual)
    sse = sum((predictions-ybar)^2)
    return (sse)
}

calculate_sst <- function(actual, predictions){
    # returns value total sum of squared errors
    ybar = mean(actual)
    sst = sum((actual - ybar)^2) 
    return (sst)
}

loglike <- function(n, sse, ssr, dfm, actual, predictions){
    # returns value for the log likelihood

    rse = sse / (n-dfm)
    nobs2 = n / 2.0
    llf = -math.log(ssr) * nobs2                   # concentrated likelihood
    llf = llf - (1 + math.log(np.pi/nobs2))*nobs2  # with likelihood constant
    llf = llf - 1/(2*math.log(rse))

    return (round(llf, 0))
}

manualAIC <- function(n, sse, ssr, dfm, actual, predictions){
    # returns value for Akaike's information criteria
    ll = loglike(n, sse, ssr, dfm, actual, predictions)
    aic = 2*(dfm+1)-2*ll
    return (aic)
}

compute_standard_error <- function(X){
    # returns calculated value of standard errors of matrix X in an array
      # - The standard errors are the diagonals of C =(X^T*V*X)^-1
      # - and v_jj = p_j * (1-p_j) and v_ij = 0 for i /= j, 1 <= i,j <= p
      # - p are the theta_hat probabilities before thresholding but after the sigmoid transformation
    cov_matrix = 1 # TODO: compute covariance matrix np.cov(X.T, rowvar=True)
    return (sqrt(diag(cov_matrix)))
}

# TODO: Convert from Python
compute_t_statistic <- function(std_err_arr, coefficients){
    # returns calculated value of t-statistics for each coefficent in an array
  
    # should instantiate an array here
    t_stat_arr = c()
    for( i in 1:length(std_err_arr)){
        t_stat = (coefficients[i] / std_err_arr[i])[0]
        t_stat_arr.append(t_stat)
    }
    return ( t_stat_arr )
}

# TODO: Convert from python
compute_p_statistic <- function(t_statistic, n){
    # returns calculated value of p-statistics for each coefficent in an array
    p_stat_arr = c()
    for( i in 1:length(t_statistic)){
        pval = 1 # stats.t.sf(np.abs(t_statistic[i]), n-1)*2
        p_stat_arr.append(pval)
    }

    return (p_stat_arr)
}
```

\subsection*{Problem 9}
```{r echo = TRUE}
# - TODO: Chose another predictive method that you think might produce similar or better results on the Framingham study.

# - Implement using library functions on the training set

# - Create confusion tables for predicting on the test set. You may use any solution parameters or tuning options.

# - Display the accuracy and the true positive and negative error rates

# - Comment on why this method did or did not result in better accuracy or a better true positive rate (precision.) No need to keep trying other methods if you didn't pick a winner initially, just explain why the method was weaker on this data set.

```

\subsection*{Problem 10}
```{r echo = FALSE}
# TODO: Make sure this is valid? 
# - Read the technical paper ==> http://circ.ahajournals.org/content/97/18/1837

# What is Logistic Regression? 
# --> Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.
```

\textbf{Question}: How is that method different from logistic regression? One paragraph is sufficient, full understanding is not required, just the essential idea.

\noindent
\textbf{Excerpt from Study}: Statistical tests included age-adjusted linear regression or logistic regression to test for trends across blood pressure, TC, LDL-C, and HDL-C categories. Age-adjusted Cox proportional hazards regression and its accompanying c statistic were used to test for the relation between various independent variables and the CHD outcome and to evaluate the discriminatory ability of various prediction models. \\

\noindent
The prediction of CHD has taken the form of sex-specific equations that were developed from a single study and applied to other populations or individuals. Age, TC, HDL-C, and blood pressure were used in the equations as continuous variables, in contrast to dichotomous variables (yes/no) such as smoking, diabetes, and left ventricular hypertrophy. The present study builds on the prior experience of CHD prediction with continuous variables and integrates the categorical approaches that have become part of the framework of blood pressure (JNC-V) and cholesterol (NCEP) programs in the United States. As suggested in an earlier NCEP report, their approach integrates blood pressure and cholesterol information and estimates both relative and absolute CHD risk with a risk factor weighting approach.
