\usepackage[obeyspaces]{url}

---
title: "Problem Set 5"
author: "Jacob Miller, Jacob Shiohira, Reid Gahan"
date: "4/14/2018"
output: pdf_document
---

```{r echo = FALSE, messages=FALSE, warning=FALSE, include=FALSE}
# NOTE: You must first install the following packages for your machine's R installation before this RMD file will compile
# install.packages("mice")
# install.packages("lattice")
# install.packages("dplyr")
# install.packages("corrplot")
# install.packages("pscl")
# install.packages("Metrics")
# install.packages("caret")
# install.packages("ROCR")
# install.packages("sigmoid")
# install.packages("randomForest")

library(mice)
library(lattice)
library(dplyr)
library(corrplot)
library(pscl)
library(Metrics)
library(caret)
library(ROCR)
library(sigmoid)
library(randomForest)

# Change as needed
workingDirectory = "D:/DataModeling2/data-and-models-ii/PROBLEM-SET-5"

setwd(workingDirectory)
# Include Keck Lib Utility Functions
source("KeckLib.R")
ECHO = TRUE
EVAL = TRUE
OUT = "750px"
SMALL = "350px"
MEDIUM = "550px"
LARGE = "750px"
date()
```

\subsection*{Problem 1}
```{r echo = TRUE}
# - Read in the Framinham data set
df <- read.csv("Data/framingham.csv", stringsAsFactors = FALSE)

# - How many observations? How many features (explanatory variables)?
str(df)
```

The output above tells us there are a total of $4,240$ observations and $16$ features in the dataset. The total number of observations in this case includes rows with possible NAN values.

\newpage

```{r echo = TRUE}
summary(df)

# - Explore the data (plots, tables, statistics) including feature types.
M <- cor(mtcars)
corrplot(M, method = "circle")
# TODO: Uncomment at the end... it just takes a while to run.
# pairs(df)

df_as_numeric <- sapply(df, as.numeric)
hist(df_as_numeric, freq=FALSE, xlab="Dataframe values", main="Distribution of Dataset Values", col="lightgreen")

# Count the total number of rows with at least one NAN value
df_nonnan <- na.omit(df)
num_null_rows <- nrow(df) - nrow(df_nonnan)
```


```{r echo = FALSE}
psi("Total number of rows with at least one NAN value: ", num_null_rows)
```


```{r echo = TRUE}
# - Summarize any NA's by feature
na_count <-sapply(df, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
```

\noindent
Below is a table outlining the number of NAN Values per feature. Note that the sum of NAN values across features is greater than the total number of rows with at least one NAN value because a single row could have multiple NAN values.

```{r echo = FALSE}
na_count
```

\subsection*{Problem 2}
```{r echo = TRUE}
n <- nrow(df)

# Documentation: https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice
# Using mice: http://web.maths.unsw.edu.au/~dwarton/missingDataLab.html, search for "pmm" method
# NOTE: changing the maxit param will affect the time it takes for mice() to run
imputed_df <- complete(mice(df, m=5, maxit=50, meth="pmm", printFlag=FALSE, seed=101))
summary(imputed_df)
```

```{r echo = TRUE}
# -- Uniformly then Normally scaling the dataset. Reference the histogram plots to verify the best type of scaling, if even necessary.
UnifDF <- ScaleUnif(imputed_df)
NormDF <- data.frame(apply(imputed_df, MARGIN = 2, FUN = function(X) (X - min(X))/diff(range(X))))
```

```{r echo = TRUE}
# -- Shuffle the dataset then split data with 80% in the training set and 20% in the test set.
set.seed(1)
shuffled_df <- NormDF[sample(n),]
trainset <- shuffled_df[1:round(0.8 * n), ]
testset <- shuffled_df[(round(0.8 * n) + 1):n, ]

# Baseline model - predict the mean of the training data with the TenYearCHD binary response variable
trainset_mean <- mean(trainset$TenYearCHD)

# Evaluate RMSE and MAE on the testing data
RMSE_baseline <- sqrt(mean((trainset_mean-testset$TenYearCHD)^2))
psp("Baseline RMSE: ", RMSE_baseline, 2)

MAE_baseline <- mean(abs(trainset_mean-testset$TenYearCHD))
psp("Baseline Mean Absolute Error: ", MAE_baseline, 2)

# Apply the "activation function" to the mean of the training set to use in all of the statistics
trainset_mean <- round(trainset_mean)

# -- What is the postive predictive rate (precision) of your baseline prediction ?
TenYearCHDAsTable <- table(testset$TenYearCHD)

truepositive <- TenYearCHDAsTable[names(TenYearCHDAsTable)==trainset_mean]
falsepositive <- TenYearCHDAsTable[names(TenYearCHDAsTable)!=trainset_mean]

psp("Postive Predictive Rate (precision): ", (truepositive / (truepositive + falsepositive)), 2)

baseline_accuracy <- Metrics::accuracy(testset$TenYearCHD, trainset_mean)
psp("Baseline Accuracy: ", baseline_accuracy, 2)
```

\noindent
\textbf{Question}: Which error type seems worse and thus should be considered along with accuracy in your predictions below? Why? \\

\noindent
The worse type of error here is not predicting cancer when there is actually cancer. It is possible that a model could have very high accuracy but just guess all $0$'s, as demonstrated by our baseline model. However, the importance in an application like this is the ability to precisely predict positive cases. Thus, the precision value, or Postive Predictive Rate, should also cbe considered alongside accuracy in the predictions below. 

\subsection*{Problem 3}
\noindent
A note on the output from \path{anova(...)}. The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time.

```{r echo = TRUE}
# Logistic Regression: http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html

# - Create a 'NULL' logistic regression model with glm() using only the intercept feature. Use glm's binomial family with the logit link.
modelNull <- glm(TenYearCHD ~ 1, family=binomial(link='logit'), data=trainset)
summary(modelNull)

modelNullAIC <- AIC(modelNull)
psi("Null Model AIC: ", modelNullAIC)

modelNullLogLike <- logLik(modelNull)
psi("Null Model Log likelihood: ", modelNullLogLike)

modelNullDeviance <- modelNull$deviance
psi("Null Model Deviance: ", modelNullDeviance)

# - Create a 'FULL' logistic regression model with glm() using all of the features including the intercept
modelFull <- glm(TenYearCHD ~., family=binomial(link='logit'), data=trainset)
summary(modelFull)

# Calculates and outputs logLik(model), model$deviance, and AIC(model) for full model
modelFullAIC <- AIC(modelFull)
psi("Full Model AIC: ", modelFullAIC)

modelFullLogLike <- logLik(modelFull)
psi("Full Model Log likelihood: ", modelFullLogLike)

modelFullDeviance <- modelFull$deviance
psi("Full Model Deviance: ", modelFullDeviance)

# - Compare the models and comment
# TODO...
```

\subsection*{Problem 4}
```{r echo = TRUE}
# - Create the lowest AIC ('best') model using backward elimination of parameters.
modelBest <- step(modelFull, direction = c("backward"), trace = 0)

# - Print the summary(), the log likelihood, the deviance, and the AIC for this model and compare with the previous two models.
summary(modelBest)

modelBestLogLike <- logLik(modelBest)
psi("Full Model Log likelihood: ", modelBestLogLike)

modelBestAIC <- AIC(modelBest)
psi("Full Model AIC: ", modelBestAIC)

# - Print these 'best' parameters (coefficients) along with their 95% confidence intervals in a single table
cbind(coef(modelBest), suppressMessages(confint(modelBest)))
```

\subsection*{Problem 5}
```{r echo = TRUE}
activationFunction <- function(x)(ifelse(x >= 0.5, 1, 0))

# - Using the 'best' model, “predict” the 10YrCHD on the training set.
trainingSetPredictions <- activationFunction(predict(modelBest, newdata=trainset, type="response"))
trainingSetPredictions <- sigmoid::sigmoid(predict(modelBest, newdata=trainset), method = "logistic", inverse = FALSE, SoftMax = FALSE)

# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates. Displayed using output function from KeckLib.R
t = table(actual = trainset$TenYearCHD, predict = trainingSetPredictions > 0.5)
acc(t, FALSE)
```

```{r echo = TRUE}
# - Compare to the baseline model from Question 2
# TODO...

# - Using the 'best' model, predict the 10YrCHD on the test set
# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates
testSetPredictions <- activationFunction(predict(modelBest, newdata=testset, type="response"))
testSetPredictions <- sigmoid::sigmoid(predict(modelBest, newdata=testset), method = "logistic", inverse = FALSE, SoftMax = FALSE)

# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates. Displayed using output function from KeckLib.R
t = table(actual = testset$TenYearCHD, predict = testSetPredictions > 0.5)
acc(t, FALSE)
```

\subsection*{Problem 6}
```{r echo = TRUE}
# Now use ROC curves to reconsider a threshold (cutoff) of .5 using the ROCR (or other) package
# -- Create the prediction object for the training set
rocrPred <- ROCR::prediction(data.frame(trainingSetPredictions)[,1], trainset$TenYearCHD)

# - Plot the accuracy v. the threshold (cutoff)
perf <- ROCR::performance(rocrPred, measure = "acc")
plot(perf, colorize=FALSE, colorize.palette=rev(rainbow(256)))
grid()

# - Plot the true positive rate v. the false positive rate and label the threshold values
tprFprPerf <- ROCR::performance(rocrPred, measure = "tpr", x.measure = "fpr") 
plot(tprFprPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()

# - Plot sensitivity v. specificity and label the threshold values
sensVsSpecPerf <- ROCR::performance(rocrPred, measure = "sens", x.measure = "spec") 
plot(sensVsSpecPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()

# - Plot precision v. recall and label the threshold values
precVsRecallPerf <- ROCR::performance(rocrPred, measure = "prec", x.measure = "rec") 
plot(precVsRecallPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()

# - Comment on your findings for each plot
# TODO...

# - TODO: Comment
# Using best
best.auc.perf <- ROCR::performance(rocrPred, measure="auc")
psr("AUC value for the 'best' model", best.auc.perf@y.values[[1]], 4)

# Create predictions using ROCR for null and full
null_trainingSetPredictions <- sigmoid::sigmoid(predict(modelNull, newdata=trainset), method = "logistic", inverse = FALSE, SoftMax = FALSE)
null_rocrPred <- ROCR::prediction(data.frame(null_trainingSetPredictions)[,1], trainset$TenYearCHD)

null.auc.perf <- ROCR::performance(null_rocrPred, measure="auc")
psr("AUC value for the 'null' model", null.auc.perf@y.values[[1]], 4)

full_trainingSetPredictions <- sigmoid::sigmoid(predict(modelFull, newdata=trainset), method = "logistic", inverse = FALSE, SoftMax = FALSE)
full_rocrPred <- ROCR::prediction(data.frame(full_trainingSetPredictions)[,1], trainset$TenYearCHD)

full.auc.perf <- ROCR::performance(full_rocrPred, measure="auc")
psr("AUC value for the 'full' model", full.auc.perf@y.values[[1]], 4)
```

\subsection*{Problem 7}
```{r echo = TRUE}
# Using gradient descent, solve for the parameters from the 'best backward eliminaton' data set and verify relative to the glm() parameters
hypothesisVector = function(X, theta) {
  # sigmoid of the dot product between X and theta
  return ( as.vector( 1 / (1+exp(-(X %*% theta))) ))
}

Jcost = function(y, h) {
  epsilon = 1.e-10
  n = nrow(y)

  # Take the error when label=0
  ifelse(h < epsilon, epsilon, h)
  # Take the error when label=1
  ifelse(h > 1-epsilon, 1-epsilon, h)

  return( (1/n)*((-y*log(h)) - ((1-y)*log(1-h))) )
}

# TODO: does not converge correctly
gradientDescent <- function(X, y){
  X = as.matrix(X)
  y = as.vector(y)
  
  theta = runif(ncol(X), -1, 1)
  alpha = 1
  maxIterations = 25000
  n = nrow(y)
  epsilon = 1.e-10
  oldSSE = 999999999999
  
  for ( i in 1:maxIterations){
      hypothesis = hypothesisVector(X, theta)
      # psi("cols: ", length(hypothesis))

      cost = Jcost(y, hypothesis)
      loss = hypothesis - y
      
      # avg gradient per example
      gradient = (t(X) %*% as.matrix(loss)) / n
      # gradient = (t(X) %*% as.matrix(hypothesis * y)) / n

      # update
      theta = theta - alpha * gradient

      newSSE = sum(loss^2)
      
      errn = abs((newSSE - oldSSE)/oldSSE)
      if(errn < epsilon){
        break
      }
      oldSSE = newSSE
  }
  
  psi("Total number of iterations: ", i)
  return (theta)
}

# - Create the X and y test and training sets
trainY <- data.frame(trainset$TenYearCHD)
trainX <- trainset[,c("male", "age", "cigsPerDay", "prevalentStroke", "prevalentHyp", "sysBP", "glucose")]
trainX <- cbind(rep(1,nrow(trainX)), trainX)
colnames(trainX)[1] = "(Intercept)"

testY <- data.frame(testset$TenYearCHD)
testX <- testset[,c("male", "age", "cigsPerDay", "prevalentStroke", "prevalentHyp", "sysBP", "glucose")]
testX <- cbind(rep(1,nrow(testX)), testX)
colnames(testX)[1] = "(Intercept)"

# - Compute the parameters (coefficients)
trainThetas <- gradientDescent(trainX, trainY)
testThetas <- gradientDescent(testX, testY)

# - Compare to glm()'s coefficients in a single table
comparisons = cbind(modelBest$coefficients[1:8], cbind(trainThetas,testThetas))
colnames(comparisons) = c("ModelBest", "GD Train", "GD Test")
comparisons
```


\subsection*{Problem 8}
```{r echo = TRUE}
# -- Compute the following from the gradient descent code in the previous problem
# - standard errors
# - Z-statistics
# - P values in a single labeled table along with the coefficients
# - The standard errors are the diagonals of C =(X^T*V*X)^-1
# - and v_jj = p_j * (1-p_j) and v_ij = 0 for i /= j, 1 <= i,j <= p
# - p are the theta_hat probabilities before thresholding but after the sigmoid transformation
# - Compute and verify the log likelihood, deviance, and AIC with those from glm()

calculate_ssr <- function(actual, predictions){
  # returns value residual sum of squared errors
  ssr = sum((actual - predictions)^2)
  return (ssr)
}

calculate_sse <- function(actual, predictions){
    # returns value sum of squared errors
    ybar = sum(actual)/nrow(actual)
    sse = sum((predictions-ybar)^2)
    return (sse)
}

calculate_sst <- function(actual, predictions){
    # returns value total sum of squared errors
    ybar = sum(actual)/nrow(actual)
    sst = sum((actual - ybar)^2) 
    return (sst)
}

LogL = function(y,h) { 
  return ( (t(y) %*% log(h) + t(1-y) %*% log(1-h)) )
}

manualAIC <- function(n, sse, ssr, dfm, actual, predictions){
    # returns value for Akaike's information criteria
    ll = loglike(n, sse, ssr, dfm, actual, predictions)
    aic = 2*(dfm+1)-2*ll
    return (aic)
}

compute_standard_error <- function(X){
    # returns calculated value of standard errors of matrix X in an array
      # - The standard errors are the diagonals of C =(X^T*V*X)^-1
      # - and v_jj = p_j * (1-p_j) and v_ij = 0 for i /= j, 1 <= i,j <= p
      # - p are the theta_hat probabilities before thresholding but after the sigmoid transformation
      #cov_matrix = cov(t(X)) # TODO: compute covariance matrix np.cov(X.T, rowvar=True)
      #return (sqrt(diag(cov_matrix)))
    mf <- function(x){sd(X[,x])/sqrt(length(X[,x]))}
    return(sapply(1:length(X[1,]),mf))
}

compute_t_statistic <- function(std_err_arr, coefficients){
    # returns calculated value of t-statistics for each coefficent in an array
  
    # should instantiate an array here
    t_stat_arr = c()
    for( i in 1:length(std_err_arr)){
        t_stat = (coefficients[i] / std_err_arr[i])
        t_stat_arr = c(t_stat_arr,t_stat)
    }
    return ( t_stat_arr )
}

compute_p_statistic <- function(t_statistic, n){
    # returns calculated value of p-statistics for each coefficent in an array
    p_stat_arr = c()
    for( i in 1:length(t_statistic)){
        pval = pt(-abs(t_statistic[i]), df = n-1) # stats.t.sf(np.abs(t_statistic[i]), n-1)*2
        p_stat_arr = c(p_stat_arr, pval)
    }

    return (p_stat_arr)
}
```

\textbf{Part 1:} Gradient Descent Statistics on Training Data

\noindent
The training data log likelihood, deviance, and AIC are pretty much the same from our own gradient descent model to the glm model's, which can be seen in the summary of question #4.
```{r}
std_err = compute_standard_error(trainX)
t_stat = compute_t_statistic(std_err, trainThetas)
p_stat = compute_p_statistic(t_stat, nrow(trainX))
train_tab <- cbind(trainThetas, cbind(std_err, cbind(t_stat, p_stat)))
colnames(train_tab) <- c("Train_Coeffs", "Std_Err_Train", "t_stat_Train", "p_stat_Train")
print(train_tab)

predictions_grad_desc_train = as.vector( 1 / (1+exp(-as.matrix(trainX) %*% as.vector(trainThetas))) )

ssr = calculate_ssr(trainY, predictions_grad_desc_train)
sse = calculate_sse(trainY, predictions_grad_desc_train)
sst = calculate_sst(trainY, predictions_grad_desc_train)
psi("SSR for training data using gradient descent: ", ssr)
psi("SSE for training data using gradient descent:", sse)
psi("SST for training data using gradient descent:", sst)

log_life_train = LogL(trainY,predictions_grad_desc_train)
dev_train = -2 * log_life_train
aic_train = dev_train + 2 * ncol(trainX)
psi("Log likelihood for training data using gradient descent: ", log_life_train)
psi("Deviance for training data using gradient descent: ", dev_train)
psi("AIC for training data using gradient descent: ", aic_train)
```

\textbf{Part 2:} Gradient Descent Statistics on Test Data
```{r}
std_err = compute_standard_error(testX)
t_stat = compute_t_statistic(std_err, testThetas)
p_stat = compute_p_statistic(t_stat, nrow(testX))
train_tab <- cbind(trainThetas, cbind(std_err, cbind(t_stat, p_stat)))
colnames(train_tab) <- c("Test_Coeffs", "Std_Err_Test", "t_stat_Test", "p_stat_Test")
print(train_tab)

predictions_grad_desc_test = as.vector( 1 / (1+exp(-as.matrix(testX) %*% as.vector(testThetas))) )

ssr = calculate_ssr(testY, predictions_grad_desc_test)
sse = calculate_sse(testY, predictions_grad_desc_test)
sst = calculate_sst(testY, predictions_grad_desc_test)
psi("SSR for testing data using gradient descent: ", ssr)
psi("SSE for testing data using gradient descent:", sse)
psi("SST for testing data using gradient descent:", sst)

log_life_test = LogL(testY,predictions_grad_desc_test)
dev_test = -2 * log_life_test
aic_test = dev_test + 2 * ncol(testX)
psi("Log likelihood for testing data using gradient descent: ", log_life_test)
psi("Deviance for testing data using gradient descent: ", dev_test)
psi("AIC for testing data using gradient descent: ", aic_test)
```


\subsection*{Problem 9}
```{r echo = TRUE}
# - Implement using library functions on the training set
fit <- randomForest(as.factor(TenYearCHD) ~., data=trainset, importance=TRUE, ntree=1000)
varImpPlot(fit)
pred <- predict(fit, testset)

# - Create confusion tables for predicting on the test set. You may use any solution parameters or tuning options.
# - Display the accuracy and the true positive and negative error rates
t = table(actual = testset$TenYearCHD, predict = pred)
acc(t, FALSE)
```

By looking at the varImpPlot, we can clearly see that some variables are much more significant than others. By not pruning the random forest and passing it all the features, some of the trees could have been made with features that were not useful, making them give improper predictions. In the situation of the Framingham study, this model is much worse because the sensitivity of it is very low.

\subsection*{Problem 10}
```{r echo = FALSE}
# TODO: Make sure this is valid? 
# - Read the technical paper ==> http://circ.ahajournals.org/content/97/18/1837

# What is Logistic Regression? 
# --> Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.
```

\textbf{Question}: How is that method different from logistic regression? One paragraph is sufficient, full understanding is not required, just the essential idea.

\noindent
\textbf{Excerpt from Study}: Statistical tests included age-adjusted linear regression or logistic regression to test for trends across blood pressure, TC, LDL-C, and HDL-C categories. Age-adjusted Cox proportional hazards regression and its accompanying c statistic were used to test for the relation between various independent variables and the CHD outcome and to evaluate the discriminatory ability of various prediction models. \\

\noindent
The prediction of CHD has taken the form of sex-specific equations that were developed from a single study and applied to other populations or individuals. Age, TC, HDL-C, and blood pressure were used in the equations as continuous variables, in contrast to dichotomous variables (yes/no) such as smoking, diabetes, and left ventricular hypertrophy. The present study builds on the prior experience of CHD prediction with continuous variables and integrates the categorical approaches that have become part of the framework of blood pressure (JNC-V) and cholesterol (NCEP) programs in the United States. As suggested in an earlier NCEP report, their approach integrates blood pressure and cholesterol information and estimates both relative and absolute CHD risk with a risk factor weighting approach.
