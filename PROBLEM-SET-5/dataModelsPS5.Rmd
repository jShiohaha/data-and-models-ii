---
title: "Problem Set 5"
author: "Jacob Miller, Jacob Shiohira, Reid Gahan"
date: "4/14/2018"
output: 
  pdf_document: 
    latex_engine: xelatex
---


```{r echo = FALSE, messages=FALSE, warning=FALSE, include=FALSE}
# NOTE: You must first install the following packages for your machine's R installation before this RMD file will compile
knitr::opts_chunk$set(echo = TRUE)

library(mice)
library(lattice)
library(dplyr)
library(corrplot)
library(pscl)
library(Metrics)
library(caret)
library(ROCR)
library(sigmoid)
library(randomForest)

# Change as needed
workingDirectory = "/Users/jacobshiohira/Documents/ACADEMICS/JUNYA/Spring/DATA-MODELS-II/PROBLEM-SETS/PROBLEM-SET-5"

setwd(workingDirectory)
# Include Keck Lib Utility Functions
source("KeckLib.R")
ECHO = TRUE
EVAL = TRUE
OUT = "750px"
SMALL = "350px"
MEDIUM = "550px"
LARGE = "750px"
date()
```

\subsection*{Problem 1}
```{r echo = TRUE}
# - Read in the Framinham data set
df <- read.csv("Data/framingham.csv", stringsAsFactors = FALSE)

# - How many observations? How many features (explanatory variables)?
str(df)
```

\noindent
The output above tells us there are a total of $4,240$ observations and $16$ features in the dataset. The total number of observations in this case includes rows with possible NAN values.

\newpage

```{r echo = TRUE}
summary(df)

# - Explore the data (plots, tables, statistics) including feature types.
M <- cor(mtcars)
corrplot(M, method = "circle")
# pairs(df)

df_as_numeric <- sapply(df, as.numeric)
hist(df_as_numeric, freq=FALSE, xlab="Dataframe values", main="Distribution of Dataset Values", col="lightgreen")

# Count the total number of rows with at least one NAN value
df_nonnan <- na.omit(df)
num_null_rows <- nrow(df) - nrow(df_nonnan)
```


```{r echo = FALSE}
psi("Total number of rows with at least one NAN value: ", num_null_rows)
```


```{r echo = TRUE}
# - Summarize any NA's by feature
na_count <-sapply(df, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
```

\noindent
Below is a table outlining the number of NAN Values per feature. Note that the sum of NAN values across features is greater than the total number of rows with at least one NAN value because a single row could have multiple NAN values.

```{r echo = FALSE}
na_count
```

\subsection*{Problem 2}
```{r echo = TRUE}
n <- nrow(df)

# Documentation: https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice
# Using mice: http://web.maths.unsw.edu.au/~dwarton/missingDataLab.html, search for "pmm" method
# NOTE: changing the maxit param will affect the time it takes for mice() to run
imputed_df <- complete(mice(df, m=5, maxit=50, meth="pmm", printFlag=FALSE, seed=101))
summary(imputed_df)
```


```{r echo = TRUE}
# -- Uniformly then Normally scaling the dataset. Reference the histogram plots to verify the best type of scaling, if even necessary.
UnifDF <- ScaleUnif(imputed_df)
NormDF <- data.frame(apply(imputed_df, MARGIN = 2, FUN = function(X) (X - min(X))/diff(range(X))))
```


```{r echo = TRUE}
# -- Shuffle the dataset then split data with 80% in the training set and 20% in the test set.
set.seed(1)
shuffled_df <- NormDF[sample(n),]
trainset <- shuffled_df[1:round(0.8 * n), ]
testset <- shuffled_df[(round(0.8 * n) + 1):n, ]

# Baseline model - predict the mean of the training data with the TenYearCHD binary response variable
trainset_mean <- mean(trainset$TenYearCHD)

# Evaluate RMSE and MAE on the testing data
RMSE_baseline <- sqrt(mean((trainset_mean-testset$TenYearCHD)^2))
psp("Baseline RMSE: ", RMSE_baseline, 2)

MAE_baseline <- mean(abs(trainset_mean-testset$TenYearCHD))
psp("Baseline Mean Absolute Error: ", MAE_baseline, 2)

# Apply the "activation function" to the mean of the training set to use in all of the statistics
trainset_mean <- round(trainset_mean)

# -- What is the postive predictive rate (precision) of your baseline prediction ?
TenYearCHDAsTable <- table(testset$TenYearCHD)

truepositive <- TenYearCHDAsTable[names(TenYearCHDAsTable)==trainset_mean]
falsepositive <- TenYearCHDAsTable[names(TenYearCHDAsTable)!=trainset_mean]

psp("Postive Predictive Rate (precision): ", (truepositive / (truepositive + falsepositive)), 2)

baseline_accuracy <- Metrics::accuracy(testset$TenYearCHD, trainset_mean)
psp("Baseline Accuracy: ", baseline_accuracy, 2)
```

\noindent
\textbf{Question}: Which error type seems worse and thus should be considered along with accuracy in your predictions below? Why? \\

\noindent
The worse type of error here is not predicting cancer when there is actually cancer. It is possible that a model could have very high accuracy but just guess all $0$'s, as demonstrated by our baseline model. However, the importance in an application like this is the ability to precisely predict positive cases. Thus, the precision value, or Postive Predictive Rate, should also cbe considered alongside accuracy in the predictions below. 

\subsection*{Problem 3}
\noindent
A note on the output from \path{anova(...)}. The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time.

```{r echo = TRUE}
# Logistic Regression: http://ww2.coastal.edu/kingw/statistics/R-tutorials/logistic.html

# - Create a 'NULL' logistic regression model with glm() using only the intercept feature. Use glm's binomial family with the logit link.
modelNull <- glm(TenYearCHD ~ 1, family=binomial(link='logit'), data=trainset)
summary(modelNull)

modelNullAIC <- AIC(modelNull)
psi("Null Model AIC: ", modelNullAIC)

modelNullLogLike <- logLik(modelNull)
psi("Null Model Log likelihood: ", modelNullLogLike)

modelNullDeviance <- modelNull$deviance
psi("Null Model Deviance: ", modelNullDeviance)

# - Create a 'FULL' logistic regression model with glm() using all of the features including the intercept
modelFull <- glm(TenYearCHD ~., family=binomial(link='logit'), data=trainset)
summary(modelFull)

# Calculates and outputs logLik(model), model$deviance, and AIC(model) for full model
modelFullAIC <- AIC(modelFull)
psi("Full Model AIC: ", modelFullAIC)

modelFullLogLike <- logLik(modelFull)
psi("Full Model Log likelihood: ", modelFullLogLike)

modelFullDeviance <- modelFull$deviance
psi("Full Model Deviance: ", modelFullDeviance)
```

\noindent
\textbf{Compare and comment on the AUC of the models}: By looking at AIC values, the 'full' logistic regression model using all the features is the better model. The null model has an AIC of 2901 while the full model has one of 2613.


\subsection*{Problem 4}
```{r echo = TRUE}
# - Create the lowest AIC ('best') model using backward elimination of parameters.
modelBest <- step(modelFull, direction = c("backward"), trace = 0)

# - Print the summary(), the log likelihood, the deviance, and the AIC for this model and compare with the previous two models.
summary(modelBest)

modelBestLogLike <- logLik(modelBest)
psi("Full Model Log likelihood: ", modelBestLogLike)

modelBestAIC <- AIC(modelBest)
psi("Full Model AIC: ", modelBestAIC)

# - Print these 'best' parameters (coefficients) along with their 95% confidence intervals in a single table
cbind(coef(modelBest), suppressMessages(confint(modelBest)))
```

\subsection*{Problem 5}
```{r echo = TRUE}
activationFunction <- function(x)(ifelse(x >= 0.5, 1, 0))

# - Using the 'best' model, “predict” the 10YrCHD on the training set.
trainingSetPredictions <- activationFunction(predict(modelBest, newdata=trainset, type="response"))
trainingSetPredictions <- sigmoid::sigmoid(predict(modelBest, newdata=trainset), method = "logistic", inverse = FALSE, SoftMax = FALSE)

# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates. Displayed using output function from KeckLib.R
t = table(actual = trainset$TenYearCHD, predict = trainingSetPredictions > 0.5)
acc(t, FALSE)
```

\noindent
\textbf{Question}: Compare to the baseline model from Question 2. \\

\noindent
The two models had the same overall accuracy of 85.14\%. This shows that even the best model still struggles to properly predict correctly.

```{r echo = TRUE}
# - Using the 'best' model, predict the 10YrCHD on the test set
# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates
testSetPredictions <- activationFunction(predict(modelBest, newdata=testset, type="response"))
testSetPredictions <- sigmoid::sigmoid(predict(modelBest, newdata=testset), method = "logistic", inverse = FALSE, SoftMax = FALSE)

# -- Display the confusion matrix, the accuracy, as well as the true positive and true negative rates. Displayed using output function from KeckLib.R
t = table(actual = testset$TenYearCHD, predict = testSetPredictions > 0.5)
acc(t, FALSE)
```

\subsection*{Problem 6}
```{r echo = TRUE}
# Now use ROC curves to reconsider a threshold (cutoff) of .5 using the ROCR (or other) package
# -- Create the prediction object for the training set
rocrPred <- ROCR::prediction(data.frame(trainingSetPredictions)[,1], trainset$TenYearCHD)
```


```{r echo = TRUE}
# - Plot the accuracy v. the threshold (cutoff)
perf <- ROCR::performance(rocrPred, measure = "acc")
plot(perf, colorize=FALSE, colorize.palette=rev(rainbow(256)))
grid()
```

\noindent
\textbf{Comment on accuracy v. the threshold}: We can see on the above graph that our model rapidly gains accuracy between the cutoff range of $0.0$ and $0.2$. However, after that point, we approach the threshold of approximately $0.85$. Thus, this tells us that there is really not much to gain in terms of accuracy past the cutoff value of $0.2$, which we should take into consideration when evaluating the ROC plots below. 

```{r echo = TRUE}
# - Plot the true positive rate v. the false positive rate and label the threshold values
tprFprPerf <- ROCR::performance(rocrPred, measure = "tpr", x.measure = "fpr") 
plot(tprFprPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()
```

\noindent
\textbf{Comment on TPR v. the FPR}: The graph above shows us that any increase in sensitivity will be accompanied by a decrease in specificity. More specifically, the model is more accurate the closer the curve follows the left-hand and top border of the ROC space. Alternatively, we see that the model is less accurate the closer the curve comes to the 45-degree diagonal of the ROC space. In the range between $0.0$ and $0.2$ on the false positive rate axis, we gain a significant amount in the value of the true positive rate without losing much of the value from the false positive rate. However, after $0.2$ on the false positive rate axis, we start to lose a more significant amount for any gain in the true positive rate. Thus, we must consider which statisitic is more important based on the context of our problem.

```{r echo = TRUE}
# - Plot sensitivity v. specificity and label the threshold values
sensVsSpecPerf <- ROCR::performance(rocrPred, measure = "sens", x.measure = "spec") 
plot(sensVsSpecPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()
```

\noindent
\textbf{Comment on sensitivity v. specificity}: Recall that sensitivity is the measure of correctly classified true values and specificity is the measure of correctly classified negative values. Thus, the above graph shows us we slowly start to misclassify true values as we rapidly start to correctly classify negative values. Depending on the application of the model, one might value a higher sensitivity measure over a higher specificity value. In this problem set, where it is objectively more important to correctly classify those with a medical condition, we value sensitivity over specificity. However, we might accept some trade off for a higher specificity value to the range of $0.4$ to $0.8$.  

```{r echo = TRUE}
# - Plot precision v. recall and label the threshold values
precVsRecallPerf <- ROCR::performance(rocrPred, measure = "prec", x.measure = "rec") 
plot(precVsRecallPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
grid()
```

\noindent
\textbf{Comment on precision v. recall}: Recall is essentially the ability of our model to find all the relevant true values within the dataset. More specifically, recall is the number of true positives divided by the number of true positives plus the number of false negatives. In contrast to recall, precision expresses the proportion of the data points our model says was relevant actually were relevant. More specifically, precision is the number of true positives divided by the number of true positives plus the number of false positives. So, in the context of the above graph, we can see there is a major point of inversion between $0.4$ and $0.3$ on the precision axis. This is the range we should consider interesting because outside of this range, the cost of tradeoff between precision and recall is too great unless we are specifically concerned with maximizing one value over the other.


```{r echo = TRUE}
# Using best
best.auc.perf <- ROCR::performance(rocrPred, measure="auc")
psr("AUC value for the 'best' model", best.auc.perf@y.values[[1]], 4)

# Create predictions using ROCR for null and full
null_trainingSetPredictions <- sigmoid::sigmoid(predict(modelNull, newdata=trainset), method = "logistic", inverse = FALSE, SoftMax = FALSE)
null_rocrPred <- ROCR::prediction(data.frame(null_trainingSetPredictions)[,1], trainset$TenYearCHD)

null.auc.perf <- ROCR::performance(null_rocrPred, measure="auc")
psr("AUC value for the 'null' model", null.auc.perf@y.values[[1]], 4)

full_trainingSetPredictions <- sigmoid::sigmoid(predict(modelFull, newdata=trainset), method = "logistic", inverse = FALSE, SoftMax = FALSE)
full_rocrPred <- ROCR::prediction(data.frame(full_trainingSetPredictions)[,1], trainset$TenYearCHD)

full.auc.perf <- ROCR::performance(full_rocrPred, measure="auc")
psr("AUC value for the 'full' model", full.auc.perf@y.values[[1]], 4)
```

\noindent
\textbf{Question}: Commment on AUC. \\

\noindent
An AUC of .5 is considered worthless, so, as expected, the 'null' model is not a desirable model for this problem. Interestingly, in our AUC calculations, the 'full' model had a higher, and therefore better, AUC than the 'best' model did, make the 'full' model the best performing model under the AUC test.

\subsection*{Problem 7}
```{r echo = TRUE}
# Using gradient descent, solve for the parameters from the 'best backward eliminaton' data set and verify relative to the glm() parameters
hypothesisVector = function(X, theta) {
  # sigmoid of the dot product between X and theta
  return ( as.vector( 1 / (1+exp(-(X %*% theta))) ))
}

Jcost = function(y, h) {
  epsilon = 0.0000000001
  n = nrow(y)

  # Take the error when label=0
  ifelse(h < epsilon, epsilon, h)
  # Take the error when label=1
  ifelse(h > 1-epsilon, 1-epsilon, h)

  return( (1/n)*((-y*log(h)) - ((1-y)*log(1-h))) )
}

gradientDescent <- function(X, y){
  X = as.matrix(X)
  y = as.vector(y)
  
  theta = runif(ncol(X), -1, 1)
  alpha = 1
  maxIterations = 25000
  n = nrow(y)
  epsilon = 0.0000000001
  oldSSE = 999999999999
  
  for ( i in 1:maxIterations){
      hypothesis = hypothesisVector(X, theta)
      # psi("cols: ", length(hypothesis))

      cost = Jcost(y, hypothesis)
      loss = hypothesis - y
      
      # avg gradient per example
      gradient = (t(X) %*% as.matrix(loss)) / n
      # gradient = (t(X) %*% as.matrix(hypothesis * y)) / n

      # update
      theta = theta - alpha * gradient

      newSSE = sum(loss^2)
      
      errn = abs((newSSE - oldSSE)/oldSSE)
      if(errn < epsilon){
        break
      }
      oldSSE = newSSE
  }
  
  psi("Total number of iterations: ", i)
  return (theta)
}

# - Create the X and y test and training sets
trainY <- data.frame(trainset$TenYearCHD)
trainX <- trainset[,c("male", "age", "cigsPerDay", "prevalentStroke", "prevalentHyp", "sysBP", "glucose")]
trainX <- cbind(rep(1,nrow(trainX)), trainX)
colnames(trainX)[1] = "(Intercept)"

testY <- data.frame(testset$TenYearCHD)
testX <- testset[,c("male", "age", "cigsPerDay", "prevalentStroke", "prevalentHyp", "sysBP", "glucose")]
testX <- cbind(rep(1,nrow(testX)), testX)
colnames(testX)[1] = "(Intercept)"

# - Compute the parameters (coefficients)
trainThetas <- gradientDescent(trainX, trainY)
testThetas <- gradientDescent(testX, testY)

# - Compare to glm()'s coefficients in a single table
comparisons = cbind(modelBest$coefficients[1:8], cbind(trainThetas,testThetas))
colnames(comparisons) = c("ModelBest", "GD Train", "GD Test")
comparisons
```


\subsection*{Problem 8}
```{r echo = TRUE}
# Section of auxiliary functions used in calculating the statistics for question 8. See the section below for the usage of these functions

calculate_ssr <- function(actual, predictions){
  # returns value residual sum of squared errors
  ssr = sum((actual - predictions)^2)
  return (ssr)
}

calculate_sse <- function(actual, predictions){
    # returns value sum of squared errors
    ybar = sum(actual)/nrow(actual)
    sse = sum((predictions-ybar)^2)
    return (sse)
}

calculate_sst <- function(actual, predictions){
    # returns value total sum of squared errors
    ybar = sum(actual)/nrow(actual)
    sst = sum((actual - ybar)^2) 
    return (sst)
}

LogL = function(y,h) { 
  return ( (t(y) %*% log(h) + t(1-y) %*% log(1-h)) )
}

manualAIC <- function(n, sse, ssr, dfm, actual, predictions){
    # returns value for Akaike's information criteria
    ll = loglike(n, sse, ssr, dfm, actual, predictions)
    aic = 2*(dfm+1)-2*ll
    return (aic)
}

compute_standard_error <- function(X){
    # returns calculated value of standard errors of matrix X in an array
    mf <- function(x){sd(X[,x])/sqrt(length(X[,x]))}
    return(sapply(1:length(X[1,]),mf))
}

compute_t_statistic <- function(std_err_arr, coefficients){
    # returns calculated value of t-statistics for each coefficent in an array
    t_stat_arr = c()
    for( i in 1:length(std_err_arr)){
        t_stat = (coefficients[i] / std_err_arr[i])
        t_stat_arr = c(t_stat_arr,t_stat)
    }
    return ( t_stat_arr )
}

compute_p_statistic <- function(t_statistic, n){
    # returns calculated value of p-statistics for each coefficent in an array
    p_stat_arr = c()
    for( i in 1:length(t_statistic)){
        pval = pt(-abs(t_statistic[i]), df = n-1) # stats.t.sf(np.abs(t_statistic[i]), n-1)*2
        p_stat_arr = c(p_stat_arr, pval)
    }

    return (p_stat_arr)
}
```

\noindent
\textbf{Part 1:} Gradient Descent Statistics on Training Data \\

\noindent
The training data log likelihood, deviance, and AIC are pretty much the same from our own gradient descent model to the glm model's, which can be seen in the summary of question #4.
```{r}
std_err = compute_standard_error(trainX)
t_stat = compute_t_statistic(std_err, trainThetas)
p_stat = compute_p_statistic(t_stat, nrow(trainX))
train_tab <- cbind(trainThetas, cbind(std_err, cbind(t_stat, p_stat)))
colnames(train_tab) <- c("Train_Coeffs", "Std_Err_Train", "t_stat_Train", "p_stat_Train")
print(train_tab)

predictions_grad_desc_train = as.vector( 1 / (1+exp(-as.matrix(trainX) %*% as.vector(trainThetas))) )

ssr = calculate_ssr(trainY, predictions_grad_desc_train)
sse = calculate_sse(trainY, predictions_grad_desc_train)
sst = calculate_sst(trainY, predictions_grad_desc_train)
psi("SSR for training data using gradient descent: ", ssr)
psi("SSE for training data using gradient descent:", sse)
psi("SST for training data using gradient descent:", sst)

log_life_train = LogL(trainY,predictions_grad_desc_train)
dev_train = -2 * log_life_train
aic_train = dev_train + 2 * ncol(trainX)
psi("Log likelihood for training data using gradient descent: ", log_life_train)
psi("Deviance for training data using gradient descent: ", dev_train)
psi("AIC for training data using gradient descent: ", aic_train)
```

\textbf{Part 2:} Gradient Descent Statistics on Test Data
```{r}
std_err = compute_standard_error(testX)
t_stat = compute_t_statistic(std_err, testThetas)
p_stat = compute_p_statistic(t_stat, nrow(testX))
train_tab <- cbind(trainThetas, cbind(std_err, cbind(t_stat, p_stat)))
colnames(train_tab) <- c("Test_Coeffs", "Std_Err_Test", "t_stat_Test", "p_stat_Test")
print(train_tab)

predictions_grad_desc_test = as.vector( 1 / (1+exp(-as.matrix(testX) %*% as.vector(testThetas))) )

ssr = calculate_ssr(testY, predictions_grad_desc_test)
sse = calculate_sse(testY, predictions_grad_desc_test)
sst = calculate_sst(testY, predictions_grad_desc_test)
psi("SSR for testing data using gradient descent: ", ssr)
psi("SSE for testing data using gradient descent:", sse)
psi("SST for testing data using gradient descent:", sst)

log_life_test = LogL(testY,predictions_grad_desc_test)
dev_test = -2 * log_life_test
aic_test = dev_test + 2 * ncol(testX)
psi("Log likelihood for testing data using gradient descent: ", log_life_test)
psi("Deviance for testing data using gradient descent: ", dev_test)
psi("AIC for testing data using gradient descent: ", aic_test)
```


\subsection*{Problem 9}
```{r echo = TRUE}
# - Implement using library functions on the training set
fit <- randomForest(as.factor(TenYearCHD) ~., data=trainset, importance=TRUE, ntree=1000)
varImpPlot(fit)
pred <- predict(fit, testset)

# - Create confusion tables for predicting on the test set. You may use any solution parameters or tuning options.
# - Display the accuracy and the true positive and negative error rates
t = table(actual = testset$TenYearCHD, predict = pred)
acc(t, FALSE)
```

\noindent
By looking at the varImpPlot, we can clearly see that some variables are much more significant than others. By not pruning the random forest and passing it all the features, some of the trees could have been made with features that were not useful, making them give improper predictions. In the situation of the Framingham study, this model is much worse because the sensitivity of it is very low.

\subsection*{Problem 10}
```{r echo = FALSE}
# - Read the technical paper ==> http://circ.ahajournals.org/content/97/18/1837
```

\noindent
\textbf{Question}: How is that method different from logistic regression? One paragraph is sufficient, full understanding is not required, just the essential idea.

\noindent
\textbf{Excerpt from Study}: Statistical tests included age-adjusted linear regression or logistic regression to test for trends across blood pressure, TC, LDL-C, and HDL-C categories. Age-adjusted Cox proportional hazards regression and its accompanying c statistic were used to test for the relation between various independent variables and the CHD outcome and to evaluate the discriminatory ability of various prediction models. \\

\noindent
The prediction of CHD has taken the form of sex-specific equations that were developed from a single study and applied to other populations or individuals. Age, TC, HDL-C, and blood pressure were used in the equations as continuous variables, in contrast to dichotomous variables (yes/no) such as smoking, diabetes, and left ventricular hypertrophy. The present study builds on the prior experience of CHD prediction with continuous variables and integrates the categorical approaches that have become part of the framework of blood pressure (JNC-V) and cholesterol (NCEP) programs in the United States. As suggested in an earlier NCEP report, their approach integrates blood pressure and cholesterol information and estimates both relative and absolute CHD risk with a risk factor weighting approach.
